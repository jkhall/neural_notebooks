{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wordlevel_seq2seq.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "o7tymjZdK8Ta",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "77739332-5a86-4f34-914a-0c998cd269a2"
      },
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/60/66415660aa46b23b5e1b72bc762e816736ce8d7260213e22365af51e8f9c/torch-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (591.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 591.8MB 25kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x6118c000 @  0x7fef41f4a2a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "Successfully installed torch-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yLOUo_sgLEmX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "6eb3d980-3afa-4e5a-c19f-9439ab2c067d"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TX_e0dVdnm_z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "ca3faa6c-a949-4bcf-8ba6-fd1eb84bd2d3"
      },
      "cell_type": "code",
      "source": [
        "!ls /gdrive/My\\ Drive/data"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " _about.txt\t\t\t fra-eng.zip\t     tarantino.csv\n",
            "'aesw2016(v1.2)_train.xml'\t fra.txt\n",
            "'aesw2016(v1.2)_train.xml.bz2'\t glove.6B.300d.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qgA678cMpt6x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "23e3ee38-eac2-4b93-db49-aa24f7e3bbb9"
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "!python -m spacy download fr"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fr_core_news_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.0.0/fr_core_news_sm-2.0.0.tar.gz#egg=fr_core_news_sm==2.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.0.0/fr_core_news_sm-2.0.0.tar.gz (39.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 39.8MB 45.7MB/s \n",
            "\u001b[?25hInstalling collected packages: fr-core-news-sm\n",
            "  Running setup.py install for fr-core-news-sm ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25hSuccessfully installed fr-core-news-sm-2.0.0\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/fr_core_news_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/fr\n",
            "\n",
            "    You can now load the model via spacy.load('fr')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_ZkEM3cKteL3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ed9842ba-0ba1-4795-f22d-2f264ceeaaef"
      },
      "cell_type": "code",
      "source": [
        "s = set()\n",
        "l1 = [\"<start>\", \"courez\", \"<end>\"]\n",
        "l2 = [\"<start>\", \"me\", \"so\", \"cool\", \"<end>\"]\n",
        "l3 = [\"thingo\", \"guy\"]\n",
        "\n",
        "s.update(l1)\n",
        "s.update(l2)\n",
        "# s.update([589,10,16,3])\n",
        "print(s)\n",
        "l1 = l3\n",
        "s.update(l1)\n",
        "l1 = l2\n",
        "s.update(l2)\n",
        "print(s)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'cool', 'me', 'so', '<end>', '<start>', 'courez'}\n",
            "{'cool', 'me', 'so', 'guy', '<end>', '<start>', 'thingo', 'courez'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0u6YOX18NG0I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# preprocessing and stuff\n",
        "PATH = \"/gdrive/My Drive/data/fra.txt\"\n",
        "import spacy\n",
        "import unicodedata\n",
        "import itertools\n",
        "\n",
        "from spacy.tokenizer import Tokenizer\n",
        "EN = spacy.load('en')\n",
        "FR = spacy.load('fr')\n",
        "\n",
        "num_samples = 1000\n",
        "\n",
        "en_tokenizer = Tokenizer(EN.vocab)\n",
        "fr_tokenizer = Tokenizer(FR.vocab)\n",
        "\n",
        "lines = \"\"\n",
        "with open(PATH) as f:\n",
        "  lines = f.read().split(\"\\n\")\n",
        "\n",
        "w2i_inp = {}\n",
        "i2w_inp = {}\n",
        "w2i_out = {}\n",
        "i2w_out = {}\n",
        "\n",
        "punc = \"?.!\"\n",
        "start_tag = \"<start>\"\n",
        "end_tag = \"<end>\"\n",
        "pad_tag = \"<pad>\"\n",
        "\n",
        "max_inp_len = 0\n",
        "max_out_len = 0\n",
        "max_inp_phrase = \"\"\n",
        "max_out_phrase = \"\"\n",
        "\n",
        "all_inp_tokens = set()\n",
        "all_out_tokens = set()\n",
        "all_tar_tokens = set()\n",
        "\n",
        "inp_seqs = []\n",
        "out_seqs = []\n",
        "\n",
        "num_inp_tokens = 0\n",
        "num_out_tokens = 0\n",
        "\n",
        "# Converts the unicode file to ascii\n",
        "# from https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb#scrollTo=rd0jw-eC3jEh\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "# what to do about start and end tags\n",
        "\n",
        "for line in lines[:num_samples]:\n",
        "  if len(line) > 1:\n",
        "    inp_txt, out_txt = line.lower().split('\\t')\n",
        "    \n",
        "    inp_txt = unicode_to_ascii(inp_txt)\n",
        "    out_txt = unicode_to_ascii(out_txt)\n",
        "    \n",
        "    inp_txt = start_tag + \" \" + inp_txt.strip(punc).strip() + \" \" + end_tag\n",
        "    out_txt = start_tag + \" \" + out_txt.strip(punc).strip() + \" \" + end_tag\n",
        "    \n",
        "    inp_tokens = list(en_tokenizer(inp_txt))\n",
        "    out_tokens = list(fr_tokenizer(out_txt))\n",
        "    \n",
        "    stringed_inp = list(map(lambda t: str(t), inp_tokens))\n",
        "    stringed_out = list(map(lambda t: str(t), out_tokens))\n",
        "    \n",
        "    if len(stringed_inp) > max_inp_len:\n",
        "      max_inp_len = len(stringed_inp)\n",
        "      max_inp_phrase = stringed_inp\n",
        "      \n",
        "    if len(stringed_out) > max_out_len:\n",
        "      max_out_len = len(stringed_out)\n",
        "      max_out_phrase = stringed_out\n",
        "    \n",
        "    inp_seqs.append(stringed_inp)\n",
        "    out_seqs.append(stringed_out)\n",
        "    \n",
        "   \n",
        "    \n",
        "    # store the entire vocab\n",
        "#     all_inp_tokens.update(*inp_tokens)\n",
        "#     all_out_tokens.update(*out_tokens)\n",
        "    all_inp_tokens.update(stringed_inp)\n",
        "    all_out_tokens.update(stringed_out)\n",
        "    \n",
        "\n",
        "      \n",
        "# print(all_out_tokens)\n",
        "# print(all_inp_tokens)\n",
        "\n",
        "# start at one because zero is padding\n",
        "for i, (inp_t, out_t) in enumerate(itertools.zip_longest(all_inp_tokens, all_out_tokens), 1):\n",
        "  \n",
        "  if inp_t != None:\n",
        "    i2w_inp[i] = inp_t\n",
        "    w2i_inp[inp_t] =  i\n",
        "    \n",
        "  if out_t != None:\n",
        "    i2w_out[i] = out_t\n",
        "    w2i_out[out_t] =  i\n",
        "  \n",
        "\n",
        "# add padding token\n",
        "i2w_inp[0] = pad_tag\n",
        "w2i_inp[pad_tag] = 0\n",
        "i2w_out[0] = pad_tag\n",
        "w2i_out[pad_tag] = 0\n",
        "    \n",
        "num_inp_tokens = len(w2i_inp)    \n",
        "num_out_tokens = len(w2i_out)    \n",
        "\n",
        "# the time delay??\n",
        "def process_seq(seqs, w2i, max_seq_len):\n",
        "  # one hot encoding for now\n",
        "  # processed = torch.zeros((num_samples, max_seq_len, len(w2i.items())))\n",
        "  processed = torch.zeros((len(seqs), max_seq_len), dtype=torch.long)\n",
        "  \n",
        "  for i, s in enumerate(seqs[-1::-1]):\n",
        "    for j, w in enumerate(s):\n",
        "      processed[i, j] = w2i[w]\n",
        "      \n",
        "  return processed\n",
        "\n",
        "# with size num_samples x max_seq_len x num_tokens\n",
        "processed_inp = process_seq(inp_seqs, w2i_inp, max_inp_len)\n",
        "processed_out = process_seq(out_seqs, w2i_out, max_out_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HyiFQ5sVGB0E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "67547934-f7e8-4764-e64e-b37d1986779b"
      },
      "cell_type": "code",
      "source": [
        "print(max_inp_len)\n",
        "print(max_inp_phrase)\n",
        "print(max_out_len)\n",
        "print(max_out_phrase)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n",
            "['<start>', 'go', '<end>']\n",
            "4\n",
            "['<start>', 'ca', 'alors', '<end>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "it6tbVMJ7x5X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "embedding_size = 200\n",
        "hidden_size = 256\n",
        "inp_vocab_size = len(w2i_inp)\n",
        "out_vocab_size = len(w2i_out)\n",
        "num_layers = 1 # tried 4 previously\n",
        "num_directions = 2\n",
        "bs = 2\n",
        "\n",
        "# get all the shapes right\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.embeds = nn.Embedding(inp_vocab_size, embedding_size)\n",
        "    self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers=num_layers, bidirectional=True)\n",
        "    \n",
        "  def forward(self, inp):\n",
        "    # max_inp_len, bs, embedding_size\n",
        "    emb = self.embeds(inp)\n",
        "    # out -> bs, max_inp_len (2 * hidden_size)\n",
        "    # reference directions with out.view(seq_len, bs, num_directions, hidden_size) -> 0 or 1 for direction\n",
        "    \n",
        "    # hs -> num_layers * num_directions, bs, hidden_size\n",
        "    # cs -> num_layers * num_directions, bs, hidden_size\n",
        "    return self.lstm(emb)\n",
        "  \n",
        "# using Bahdanau attention\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.embeds = nn.Embedding(out_vocab_size, embedding_size)\n",
        "    self.tanh = nn.Tanh()\n",
        "    self.v = nn.Linear(128, 1)\n",
        "    self.w1 = nn.Linear(num_directions * hidden_size, 128)\n",
        "    self.w2 = nn.Linear(hidden_size, 128)\n",
        "    \n",
        "    \n",
        "    self.d = nn.Linear(hidden_size, out_vocab_size) # to get into the right shape\n",
        "    self.lstm = nn.LSTM(num_directions * hidden_size + embedding_size, hidden_size, num_layers=num_layers)\n",
        "    \n",
        "    # what is hs versus ht\n",
        "    # you probably have to feed this one at a time\n",
        "  def forward(self, x, state, enc_outputs):\n",
        "    # enc_outputs -> bs, max_inp_len, num_directions * hidden_size\n",
        "    # x -> 1\n",
        "    # h -> max_inp_len, hidden_size <- transform this? I want something 2d\n",
        "    # w1 * enc_outputs -> bs, max_inp_len, 128\n",
        "    # w2 * h -> max_inp_len, 128\n",
        "    # N = (num_layers * num_directions)\n",
        "    # cat_vec -> bs, max_inp_len 128\n",
        "    # scores -> bs, max_inp_len  1\n",
        "    # attn_weights -> bs, max_inp_len, 1\n",
        "    hidden = state[0]\n",
        "    h = torch.mean(hidden, 0)\n",
        "    cat_vec = self.w1(enc_outputs) + self.w2(h).unsqueeze(0) #which dim?\n",
        "    scores = self.v(self.tanh(cat_vec))\n",
        "    attn_weights = F.log_softmax(scores, dim=1)\n",
        "    \n",
        "#     print(\"hidden: {}\".format(hidden.shape))\n",
        "#     print(\"enc out: {}\".format(enc_outputs.shape))\n",
        "#     print(\"attn weights: {}\".format(attn_weights.shape))\n",
        "    \n",
        "    # bs, max_inp_len, num_directions * hidden_size\n",
        "    context = attn_weights * enc_outputs\n",
        "    context = torch.sum(context, 1).view(enc_outputs.shape[0], 1, -1)\n",
        "    \n",
        "#     print(\"context shape: {}\".format(context.shape))\n",
        "    \n",
        "    # bs, 1, embedding_size\n",
        "    x_emb = self.embeds(x).unsqueeze(1)\n",
        "    \n",
        "    \n",
        "    # bs, 1, embedding_size + num_directions * hidden_size\n",
        "    x_emb_and_context = torch.cat((x_emb, context), -1)\n",
        "    \n",
        "#     print(x_emb_and_context.shape)\n",
        "    \n",
        "    # bs, 1, num_directions * hidden_size\n",
        "    out, (hs, cs) = self.lstm(x_emb_and_context)\n",
        "    \n",
        "    # bs, 1, out_vocab_size\n",
        "    out = self.d(out)\n",
        "\n",
        "    \n",
        "    return F.log_softmax(out, dim=-1), (hs, cs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iaG9rwiu_bbq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "be28b071-76da-49ea-df14-6f2ae4464b71"
      },
      "cell_type": "code",
      "source": [
        "print(max_inp_len)\n",
        "print(max_out_len)\n",
        "\n",
        "print(num_samples // 2)"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n",
            "9\n",
            "500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "h03OuuHIiwkb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "f8ebe2bd-d14e-4bc5-f73e-7a99ed56cdc6"
      },
      "cell_type": "code",
      "source": [
        "enc = Encoder()\n",
        "dec = Decoder()\n",
        "\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "accuracy = 0\n",
        "optimizer = torch.optim.RMSprop([*enc.parameters(), *dec.parameters()], lr=0.0005)\n",
        "\n",
        "# split the data into a validation set\n",
        "# 20%? <- I'm not sure that works here, given, I don't think this will generalize...?\n",
        "# basically, the system can't be expected to generate a translation for things it's never seen before...\n",
        "def validation(epoch, teacher_forcing=True):\n",
        "  # not sure yet\n",
        "  return\n",
        "\n",
        "def train(epoch, teacher_forcing=True):\n",
        "  total_loss = 0\n",
        "  \n",
        "  for i in range((num_samples) // bs):\n",
        "    \n",
        "    batch_input = processed_inp[i*bs:i*bs + bs]\n",
        "    batch_output = processed_out[i*bs:i*bs + bs]\n",
        "    \n",
        "    enc_outputs, (hs, cs) = enc(batch_input)\n",
        "    \n",
        "    dec_input = batch_output[:,0] #start tags\n",
        "    \n",
        "    loss = 0\n",
        "    \n",
        "    for t in range(1, max_out_len):\n",
        "      dec_out, (hs, cs) = dec(dec_input, (hs, cs), enc_outputs)\n",
        "      \n",
        "      loss += loss_func(dec_out.squeeze(1), batch_output[:,t])\n",
        "\n",
        "      if teacher_forcing:\n",
        "        dec_input = batch_output[:,t] #feed targets\n",
        "      else:\n",
        "        dec_input = dec_out #feed predictions\n",
        "      \n",
        "      \n",
        "    loss.backward()\n",
        "    \n",
        "    total_loss += loss.item()\n",
        "    \n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "  return total_loss\n",
        "    \n",
        "epochs = 200\n",
        "\n",
        "for e in range(epochs):\n",
        "  all_loss = train(e, teacher_forcing=True)\n",
        "  \n",
        "  if e % 10 == 0:\n",
        "    print(all_loss)"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7348.703876018524\n",
            "523.3971020504832\n",
            "196.82568502845243\n",
            "157.39631367329275\n",
            "140.0397917900409\n",
            "134.64355736157813\n",
            "126.34079016406417\n",
            "125.65663608152897\n",
            "134.36963966869484\n",
            "123.47220421350539\n",
            "124.0023149696338\n",
            "131.8164099638384\n",
            "133.40924191668947\n",
            "125.56055317252326\n",
            "140.52100048771132\n",
            "116.55861834327779\n",
            "116.85155664656153\n",
            "118.9114346204124\n",
            "115.85045656959122\n",
            "117.14848609665398\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-fWUic0zpHy7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c023e683-db4e-470c-dfca-31c6803f97ab"
      },
      "cell_type": "code",
      "source": [
        "# decode\n",
        "# s = inp_seqs[6]\n",
        "# print(s)\n",
        "\n",
        "# eng seq\n",
        "def process_seq(seq):\n",
        "  uni_seq = unicode_to_ascii(seq)\n",
        "  uni_seq = start_tag + \" \" + uni_seq.strip(punc).strip() + \" \" + end_tag\n",
        "  processed = list(en_tokenizer(uni_seq))\n",
        "  processed = map(lambda s: str(s), processed)\n",
        "#   processed = s\n",
        "  \n",
        "  tensor = torch.zeros((1, max_inp_len), dtype=torch.long)\n",
        "  \n",
        "  for i, w in enumerate(processed):\n",
        "    tensor[:, i] = w2i_inp[w]\n",
        "    \n",
        "#   print(tensor)\n",
        "  \n",
        "  translation = \"\"\n",
        "  meta_trans = [start_tag]\n",
        "  idx = 0\n",
        "  enc_out, (hs, cs) = enc(tensor)\n",
        "  dec_inp = torch.tensor([w2i_inp[start_tag]])\n",
        "  \n",
        "#   print(dec_inp.shape)\n",
        "#   print(enc_out.shape)\n",
        "  \n",
        "  while idx < max_out_len:\n",
        "    dec_out, (hs, cs) = dec(dec_inp, (hs, cs), enc_out)\n",
        "    word_idx = torch.argmax(dec_out, dim=-1)\n",
        "    word = i2w_out[int(word_idx[0][0])]\n",
        "    meta_trans.append(word)\n",
        "#     print(word_idx[0].shape)\n",
        "    dec_inp = word_idx[0] \n",
        "    if word == end_tag or word == pad_tag:\n",
        "#       print(\"produced end/pad tag\")\n",
        "      return translation.strip(), meta_trans # we're done!\n",
        "    else:\n",
        "      translation += (word + \" \")\n",
        "    idx += 1\n",
        "  return translation.strip(), meta_trans\n",
        "      \n",
        "      \n",
        "t, m = process_seq(\"\")\n",
        "print(t)\n",
        "print(m)"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pars d'ici\n",
            "['<start>', 'pars', \"d'ici\", '<end>']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}